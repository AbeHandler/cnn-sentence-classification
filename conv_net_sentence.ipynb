{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample code for\n",
    "Convolutional Neural Networks for Sentence Classification\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "\n",
    "Much of the code is modified from\n",
    "- deeplearning.net (for ConvNet classes)\n",
    "- https://github.com/mdenil/dropout (for dropout)\n",
    "- https://groups.google.com/forum/#!topic/pylearn-dev/3QbKtCumAW4 (for Adadelta)\n",
    "\"\"\"\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    y = T.maximum(0.0, x)\n",
    "    return(y)\n",
    "def Sigmoid(x):\n",
    "    y = T.nnet.sigmoid(x)\n",
    "    return(y)\n",
    "def Tanh(x):\n",
    "    y = T.tanh(x)\n",
    "    return(y)\n",
    "def Iden(x):\n",
    "    y = x\n",
    "    return(y)\n",
    "       \n",
    "def train_conv_net(datasets, U, img_w=300, filter_hs=[3,4,5], hidden_units=[100,2],  dropout_rate=[0.5], shuffle_batch=True,\n",
    "                   n_epochs=25, batch_size=50, lr_decay = 0.95, conv_non_linear=\"relu\", activations=[Iden], sqr_norm_lim=9,\n",
    "                   non_static=True):\n",
    "    \"\"\"\n",
    "    Train a simple conv net\n",
    "    img_h = sentence length (padded where necessary)\n",
    "    img_w = word vector length (300 for word2vec)\n",
    "    filter_hs = filter window sizes    \n",
    "    hidden_units = [x,y] x is the number of feature maps (per filter window), and y is the penultimate layer\n",
    "    sqr_norm_lim = s^2 in the paper\n",
    "    lr_decay = adadelta decay parameter\n",
    "    \"\"\"    \n",
    "    rng = np.random.RandomState(3435)\n",
    "    img_h = len(datasets[0][0])-1  \n",
    "    filter_w = img_w    \n",
    "    feature_maps = hidden_units[0]\n",
    "    filter_shapes = []\n",
    "    pool_sizes = []\n",
    "    for filter_h in filter_hs:\n",
    "        filter_shapes.append((feature_maps, 1, filter_h, filter_w))\n",
    "        pool_sizes.append((img_h-filter_h+1, img_w-filter_w+1))\n",
    "    parameters = [(\"image shape\",img_h,img_w),(\"filter shape\",filter_shapes), (\"hidden_units\",hidden_units),\n",
    "                  (\"dropout\", dropout_rate), (\"batch_size\",batch_size),(\"non_static\", non_static),\n",
    "                    (\"learn_decay\",lr_decay), (\"conv_non_linear\", conv_non_linear), (\"non_static\", non_static)\n",
    "                    ,(\"sqr_norm_lim\",sqr_norm_lim),(\"shuffle_batch\",shuffle_batch)]\n",
    "    print parameters    \n",
    "    \n",
    "    #define model architecture\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')   \n",
    "    y = T.ivector('y')\n",
    "    Words = theano.shared(value = U, name = \"Words\")\n",
    "    zero_vec_tensor = T.vector()\n",
    "    zero_vec = np.zeros(img_w)\n",
    "    set_zero = theano.function([zero_vec_tensor], updates=[(Words, T.set_subtensor(Words[0,:], \n",
    "                                zero_vec_tensor))], allow_input_downcast=True)\n",
    "    layer0_input = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((x.shape[0],1,x.shape[1],\n",
    "                                                                     Words.shape[1]))                                  \n",
    "    conv_layers = []\n",
    "    layer1_inputs = []\n",
    "    for i in xrange(len(filter_hs)):\n",
    "        filter_shape = filter_shapes[i]\n",
    "        pool_size = pool_sizes[i]\n",
    "        conv_layer = LeNetConvPoolLayer(rng, input=layer0_input,image_shape=(batch_size, 1, img_h, img_w),\n",
    "                                filter_shape=filter_shape, poolsize=pool_size, non_linear=conv_non_linear)\n",
    "        layer1_input = conv_layer.output.flatten(2)\n",
    "        conv_layers.append(conv_layer)\n",
    "        layer1_inputs.append(layer1_input)\n",
    "    layer1_input = T.concatenate(layer1_inputs,1)\n",
    "    hidden_units[0] = feature_maps*len(filter_hs)    \n",
    "    classifier = MLPDropout(rng, input=layer1_input, layer_sizes=hidden_units, activations=activations, \n",
    "                            dropout_rates=dropout_rate)\n",
    "    \n",
    "    #define parameters of the model and update functions using adadelta\n",
    "    params = classifier.params     \n",
    "    for conv_layer in conv_layers:\n",
    "        params += conv_layer.params\n",
    "    if non_static:\n",
    "        #if word vectors are allowed to change, add them as model parameters\n",
    "        params += [Words]\n",
    "    cost = classifier.negative_log_likelihood(y) \n",
    "    dropout_cost = classifier.dropout_negative_log_likelihood(y)           \n",
    "    grad_updates = sgd_updates_adadelta(params, dropout_cost, lr_decay, 1e-6, sqr_norm_lim)\n",
    "    \n",
    "    #shuffle dataset and assign to mini batches. if dataset size is not a multiple of mini batches, replicate \n",
    "    #extra data (at random)\n",
    "    np.random.seed(3435)\n",
    "    if datasets[0].shape[0] % batch_size > 0:\n",
    "        extra_data_num = batch_size - datasets[0].shape[0] % batch_size\n",
    "        train_set = np.random.permutation(datasets[0])   \n",
    "        extra_data = train_set[:extra_data_num]\n",
    "        new_data=np.append(datasets[0],extra_data,axis=0)\n",
    "    else:\n",
    "        new_data = datasets[0]\n",
    "    new_data = np.random.permutation(new_data)\n",
    "    \n",
    "    n_batches = new_data.shape[0]/batch_size\n",
    "    n_train_batches = int(np.round(n_batches*0.9))\n",
    "   \n",
    "    train_set = new_data[:n_batches*batch_size,:]\n",
    "    val_set = datasets[1]\n",
    "    test_set_x = datasets[2]\n",
    "\n",
    "    \n",
    "    train_set_x, train_set_y = shared_dataset((train_set[:,:img_h],train_set[:,-1]))\n",
    "    val_set_x, val_set_y = shared_dataset((val_set[:,:img_h],val_set[:,-1]))\n",
    "    n_val_batches = n_batches - n_train_batches\n",
    "    \n",
    "    get_acc_val_model = theano.function([index], classifier.errors(y),\n",
    "        givens={\n",
    "            x: val_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: val_set_y[index * batch_size: (index + 1) * batch_size]},\n",
    "                                allow_input_downcast=True)\n",
    "            \n",
    "    #compile theano functions to get train/val/test errors\n",
    "    get_acc_train_model = theano.function([index], classifier.errors(y),\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: train_set_y[index * batch_size: (index + 1) * batch_size]},\n",
    "                                allow_input_downcast=True)               \n",
    "    train_model = theano.function([index], cost, updates=grad_updates,\n",
    "        givens={\n",
    "            x: train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                y: train_set_y[index*batch_size:(index+1)*batch_size]},\n",
    "                                allow_input_downcast = True)     \n",
    "    test_pred_layers_a = []\n",
    "    test_pred_layers_b = []\n",
    "    \n",
    "    \n",
    "    # If test set is large loading the whole test set can give GPU memory allocation error\n",
    "    # So we make prediction only by taking a maximum of 2000 test examples at a time\n",
    "    test_size = test_set_x.shape[0]\n",
    "    test_batch_size = 2000\n",
    "    test_iter = int(test_size/test_batch_size)\n",
    "    extra_test_size = test_size - test_iter * test_batch_size\n",
    "    \n",
    "    test_layer0_input_a = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((test_batch_size,1,img_h,Words.shape[1]))\n",
    "    for conv_layer in conv_layers:\n",
    "        test_layer0_output_a = conv_layer.predict(test_layer0_input_a, test_batch_size)\n",
    "        test_pred_layers_a.append(test_layer0_output_a.flatten(2))\n",
    "    test_layer1_input_a = T.concatenate(test_pred_layers_a, 1)\n",
    "    test_y_pred_a = classifier.predict(test_layer1_input_a)\n",
    "    \n",
    "    \n",
    "    test_layer0_input_b = Words[T.cast(x.flatten(),dtype=\"int32\")].reshape((extra_test_size,1,img_h,Words.shape[1]))\n",
    "    for conv_layer in conv_layers:\n",
    "        test_layer0_output_b = conv_layer.predict(test_layer0_input_b, extra_test_size)\n",
    "        test_pred_layers_b.append(test_layer0_output_b.flatten(2))\n",
    "    test_layer1_input_b = T.concatenate(test_pred_layers_b, 1)\n",
    "    test_y_pred_b = classifier.predict(test_layer1_input_b)\n",
    "    \n",
    "    \n",
    "    test_model_all_a = theano.function([x], test_y_pred_a, allow_input_downcast = True)   \n",
    "    test_model_all_b = theano.function([x], test_y_pred_b, allow_input_downcast = True) \n",
    "    \n",
    "    #start training over mini-batches\n",
    "    print '... training'\n",
    "    epoch = 0\n",
    "    best_val_perf = 0\n",
    "    val_perf = 0\n",
    "    test_perf = 0       \n",
    "    cost_epoch = 0  \n",
    "    while (epoch < n_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch = epoch + 1\n",
    "        if shuffle_batch:\n",
    "            for minibatch_index in np.random.permutation(range(n_train_batches)):\n",
    "                cost_epoch = train_model(minibatch_index)\n",
    "                set_zero(zero_vec)\n",
    "        else:\n",
    "            for minibatch_index in xrange(n_train_batches):\n",
    "                cost_epoch = train_model(minibatch_index)  \n",
    "                set_zero(zero_vec)\n",
    "        train_losses = [get_acc_train_model(i) for i in xrange(n_train_batches)]\n",
    "        train_perf = 1 - np.mean(train_losses)\n",
    "        val_losses = [get_acc_val_model(i) for i in xrange(n_val_batches)]\n",
    "        val_perf = 1- np.mean(val_losses)                        \n",
    "        print('epoch: %i, training time: %.2f secs, train perf: %.2f %%, val perf: %.2f %%' % (epoch, \n",
    "                                         time.time()-start_time, train_perf * 100., val_perf*100.))\n",
    "\n",
    "    prediction1 = np.zeros((test_iter, test_batch_size))\n",
    "    for j in xrange(test_iter):\n",
    "        prediction1[j] = test_model_all_a(test_set_x[test_batch_size*j:test_batch_size*(j+1) , :])\n",
    "        \n",
    "    prediction2 = test_model_all_b(test_set_x[-extra_test_size: , :])    \n",
    "    prediction = list(prediction1.reshape(test_batch_size * test_iter)) + list(prediction2)\n",
    "    return prediction\n",
    "        \n",
    "def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "        \n",
    "def sgd_updates_adadelta(params,cost,rho=0.95,epsilon=1e-6,norm_lim=9,word_vec_name='Words'):\n",
    "    \"\"\"\n",
    "    adadelta update rule, mostly from\n",
    "    https://groups.google.com/forum/#!topic/pylearn-dev/3QbKtCumAW4 (for Adadelta)\n",
    "    \"\"\"\n",
    "    updates = OrderedDict({})\n",
    "    exp_sqr_grads = OrderedDict({})\n",
    "    exp_sqr_ups = OrderedDict({})\n",
    "    gparams = []\n",
    "    for param in params:\n",
    "        empty = np.zeros_like(param.get_value())\n",
    "        exp_sqr_grads[param] = theano.shared(value=as_floatX(empty),name=\"exp_grad_%s\" % param.name)\n",
    "        gp = T.grad(cost, param)\n",
    "        exp_sqr_ups[param] = theano.shared(value=as_floatX(empty), name=\"exp_grad_%s\" % param.name)\n",
    "        gparams.append(gp)\n",
    "    for param, gp in zip(params, gparams):\n",
    "        exp_sg = exp_sqr_grads[param]\n",
    "        exp_su = exp_sqr_ups[param]\n",
    "        up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp)\n",
    "        updates[exp_sg] = up_exp_sg\n",
    "        step =  -(T.sqrt(exp_su + epsilon) / T.sqrt(up_exp_sg + epsilon)) * gp\n",
    "        updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step)\n",
    "        stepped_param = param + step\n",
    "        if (param.get_value(borrow=True).ndim == 2) and (param.name!='Words'):\n",
    "            col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0))\n",
    "            desired_norms = T.clip(col_norms, 0, T.sqrt(norm_lim))\n",
    "            scale = desired_norms / (1e-7 + col_norms)\n",
    "            updates[param] = stepped_param * scale\n",
    "        else:\n",
    "            updates[param] = stepped_param      \n",
    "    return updates \n",
    "\n",
    "def as_floatX(variable):\n",
    "    if isinstance(variable, float):\n",
    "        return np.cast[theano.config.floatX](variable)\n",
    "\n",
    "    if isinstance(variable, np.ndarray):\n",
    "        return np.cast[theano.config.floatX](variable)\n",
    "    return theano.tensor.cast(variable, theano.config.floatX)\n",
    "    \n",
    "def safe_update(dict_to, dict_from):\n",
    "    \"\"\"\n",
    "    re-make update dictionary for safe updating\n",
    "    \"\"\"\n",
    "    for key, val in dict(dict_from).iteritems():\n",
    "        if key in dict_to:\n",
    "            raise KeyError(key)\n",
    "        dict_to[key] = val\n",
    "    return dict_to\n",
    "    \n",
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in xrange(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l+2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_train_valid_data(train_revs, word_idx_map, cv, max_l = 51, k = 300, filter_h = 5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, valid = [], []\n",
    "    for rev in train_revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h)   \n",
    "        sent.append(rev[\"y\"])\n",
    "        if rev[\"split\"]==cv:            \n",
    "            valid.append(sent)        \n",
    "        else:  \n",
    "            train.append(sent)\n",
    " \n",
    "    train = np.array(train,dtype=\"int\")\n",
    "    valid = np.array(valid,dtype='int' )\n",
    "    return [train, valid]    \n",
    "\n",
    "def make_idx_test_data(test_revs, word_idx_map, cv, max_l = 51, k = 300, filter_h = 5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    test = []\n",
    "    for rev in test_revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h)   \n",
    "        test.append(sent)  \n",
    "        \n",
    "    test = np.array(test,dtype=\"int\")\n",
    "    return test     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... data loaded!\n",
      "Model architecture: CNN-non-static\n",
      "Using: Word2Vec \n"
     ]
    }
   ],
   "source": [
    "print 'Loading data...',\n",
    "x = cPickle.load(open(\"mr.p\",\"rb\"))\n",
    "train_revs, test_revs, W1, W2, W3, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4], x[5], x[6]\n",
    "print \"data loaded!\"\n",
    "    \n",
    "mode= 'nonstatic'\n",
    "word_vectors = 'Word2Vec'\n",
    "    \n",
    "    \n",
    "if mode==\"nonstatic\":\n",
    "    print \"Model architecture: CNN-non-static\"\n",
    "    non_static=True\n",
    "elif mode==\"static\":\n",
    "    print \"Model architecture: CNN-static\"\n",
    "    non_static=False\n",
    "    \n",
    "execfile(\"conv_net_classes.py\")    \n",
    "\n",
    "if word_vectors==\"Word2Vec\":\n",
    "    print \"Using: Word2Vec \"\n",
    "    U = W1\n",
    "elif word_vectors==\"GloVe\":\n",
    "    print \"Using: GloVe \"\n",
    "    U = W2\n",
    "elif word_vectors=='Random':\n",
    "    print \"Using: Random vectors\"\n",
    "    U = W3\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('image shape', 64, 300), ('filter shape', [(16, 1, 3, 300), (16, 1, 4, 300), (16, 1, 5, 300)]), ('hidden_units', [16, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch: 1, training time: 40.19 secs, train perf: 80.16 %, val perf: 76.74 %\n",
      "epoch: 2, training time: 41.82 secs, train perf: 85.60 %, val perf: 78.11 %\n",
      "Test Accuracy  : 88.0 %\n",
      "\n",
      "[('image shape', 64, 300), ('filter shape', [(16, 1, 3, 300), (16, 1, 4, 300), (16, 1, 5, 300)]), ('hidden_units', [16, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch: 1, training time: 43.09 secs, train perf: 76.18 %, val perf: 77.79 %\n",
      "epoch: 2, training time: 43.27 secs, train perf: 85.11 %, val perf: 80.00 %\n",
      "Test Accuracy  : 86.0 %\n",
      "\n",
      "[('image shape', 64, 300), ('filter shape', [(16, 1, 3, 300), (16, 1, 4, 300), (16, 1, 5, 300)]), ('hidden_units', [16, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch: 1, training time: 43.65 secs, train perf: 76.78 %, val perf: 79.05 %\n",
      "epoch: 2, training time: 43.62 secs, train perf: 83.37 %, val perf: 80.32 %\n",
      "Test Accuracy  : 87.0 %\n",
      "\n",
      "[('image shape', 64, 300), ('filter shape', [(16, 1, 3, 300), (16, 1, 4, 300), (16, 1, 5, 300)]), ('hidden_units', [16, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch: 1, training time: 40.01 secs, train perf: 77.21 %, val perf: 77.37 %\n",
      "epoch: 2, training time: 40.23 secs, train perf: 83.76 %, val perf: 77.89 %\n",
      "Test Accuracy  : 89.0 %\n",
      "\n",
      "[('image shape', 64, 300), ('filter shape', [(16, 1, 3, 300), (16, 1, 4, 300), (16, 1, 5, 300)]), ('hidden_units', [16, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
      "... training\n",
      "epoch: 1, training time: 39.88 secs, train perf: 76.53 %, val perf: 74.74 %\n",
      "epoch: 2, training time: 39.70 secs, train perf: 83.59 %, val perf: 76.63 %\n",
      "Test Accuracy  : 86.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = np.array(list(np.zeros(50)) + list(np.ones(50)))   # the actual test classes\n",
    "y_prediction = np.zeros((10, 100))                          # initializing y_prediction with all zeros\n",
    "test = make_idx_test_data(test_revs, word_idx_map, i, max_l = 56, k = 300, filter_h = 5)\n",
    "\n",
    "r = range(0,5)    \n",
    "\n",
    "for i in r:\n",
    "    datasets = make_idx_train_valid_data(train_revs, word_idx_map, i, max_l = 56, k = 300, filter_h = 5)\n",
    "    datasets.append(test)\n",
    "    y_prediction = train_conv_net(datasets,U, lr_decay=0.95, filter_hs=[3,4,5], conv_non_linear=\"relu\", hidden_units=[16,2], \n",
    "                            shuffle_batch=True, n_epochs=2, sqr_norm_lim=9, non_static=non_static, batch_size=50,\n",
    "                            dropout_rate=[0.5])\n",
    "    \n",
    "\n",
    "    acc = np.sum(y_test == y_prediction, axis = 0) * 100/float(len(y_test))\n",
    "    print 'Test Accuracy ' + ' : ' + str(acc) + ' %'\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
